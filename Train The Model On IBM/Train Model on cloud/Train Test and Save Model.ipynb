{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  \n",
    "from** google.colab **import** drive\n",
    "\n",
    "drive**.**mount('/content/drive')\n",
    "\n",
    "Mounted at /content/drive\n",
    "\n",
    "In \\[8\\]:\n",
    "\n",
    "ls\n",
    "\n",
    "**drive**/ **sample\\_data**/\n",
    "\n",
    "In \\[9\\]:\n",
    "\n",
    "cd **/**content**/**drive**/**MyDrive**/**dataset\n",
    "\n",
    "/content/drive/MyDrive/dataset\n",
    "\n",
    "In \\[10\\]:\n",
    "\n",
    "ls\n",
    "\n",
    "readme.txt **test\\_set**/ **train\\_set**/\n",
    "\n",
    "In \\[11\\]:\n",
    "\n",
    "pwd\n",
    "\n",
    "Out\\[11\\]:\n",
    "\n",
    "'/content/drive/MyDrive/dataset'\n",
    "\n",
    "In \\[12\\]:\n",
    "\n",
    "*\\# importing imagedatagenerator library*\n",
    "\n",
    "**from** tensorflow.keras.preprocessing.image **import**\n",
    "ImageDataGenerator\n",
    "\n",
    "**Image Data Agumentation**\n",
    "\n",
    "In \\[13\\]:\n",
    "\n",
    "*\\#Configuring image Data Generator Class*\n",
    "\n",
    "*\\#Setting Parameter for Image Augmentation for training data*\n",
    "\n",
    "train\\_datagen**=**ImageDataGenerator(rescale**=**1.**/**255,horizontal\\_flip**=True**,vertical\\_flip**=True**,\n",
    "zoom\\_range**=**0.2)\n",
    "\n",
    "In \\[14\\]:\n",
    "\n",
    "*\\#Image Data Augmentation for testing data*\n",
    "\n",
    "test\\_datagen**=** ImageDataGenerator(rescale**=**1.**/**255)\n",
    "\n",
    "**Apply ImageDataGenerator Functionality To Train And Test Dataset**\n",
    "\n",
    "In \\[15\\]:\n",
    "\n",
    "*\\#Performing data augmentation to train data*\n",
    "\n",
    "x\\_train **=**\n",
    "train\\_datagen**.**flow\\_from\\_directory('/content/drive/MyDrive/dataset/train\\_set',\n",
    "target\\_size **=** (64,64), batch\\_size **=** 5, color\\_mode **=**\n",
    "'rgb', class\\_mode **=** 'categorical')\n",
    "\n",
    "Found 744 images belonging to 4 classes.\n",
    "\n",
    "In \\[16\\]:\n",
    "\n",
    "*\\#performing data augmentation to test data*\n",
    "\n",
    "x\\_test **=**\n",
    "test\\_datagen**.**flow\\_from\\_directory('/content/drive/MyDrive/dataset/test\\_set',\n",
    "target\\_size **=** (64,64), batch\\_size **=** 5, color\\_mode **=**\n",
    "'rgb', class\\_mode **=** 'categorical')\n",
    "\n",
    "Found 198 images belonging to 4 classes.\n",
    "\n",
    "In \\[17\\]:\n",
    "\n",
    "*\\#importing neccessary libraries*\n",
    "\n",
    "**import** numpy **as** np\n",
    "\n",
    "**import** tensorflow\n",
    "\n",
    "**from** tensorflow.keras.models **import** Sequential\n",
    "\n",
    "**from** tensorflow.keras.layers **import**\n",
    "Dense,Conv2D,MaxPooling2D,Flatten\n",
    "\n",
    "In \\[18\\]:\n",
    "\n",
    "*\\# initialising the model and adding CNN layers*\n",
    "\n",
    "model **=** Sequential()\n",
    "\n",
    "*\\# First convolution layer and pooling*\n",
    "\n",
    "model**.**add(Conv2D(32,(3,3),input\\_shape**=**(64,64,3),activation**=**'relu'))\n",
    "\n",
    "model**.**add(MaxPooling2D(pool\\_size**=**(2,2)))\n",
    "\n",
    "*\\#Second convolution layer and pooling*\n",
    "\n",
    "model**.**add(Conv2D(32,(3,3),activation**=**'relu'))\n",
    "\n",
    "model**.**add(MaxPooling2D(pool\\_size**=**(2,2)))\n",
    "\n",
    "*\\#Flattening the layers*\n",
    "\n",
    "model**.**add(Flatten())\n",
    "\n",
    "*\\#Adding Dense Layers*\n",
    "\n",
    "model**.**add(Dense(units**=**128,activation**=**'relu'))\n",
    "\n",
    "model**.**add(Dense(units**=**4,activation**=**'softmax'))\n",
    "\n",
    "In \\[19\\]:\n",
    "\n",
    "*\\# Summary of our model*\n",
    "\n",
    "model**.**summary()\n",
    "\n",
    "Model: \"sequential\\_1\"\n",
    "\n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "\n",
    "Layer (type) Output Shape Param \\#\n",
    "\n",
    "=================================================================\n",
    "\n",
    "conv2d\\_2 (Conv2D) (None, 62, 62, 32) 896\n",
    "\n",
    "max\\_pooling2d\\_2 (MaxPooling (None, 31, 31, 32) 0\n",
    "\n",
    "2D)\n",
    "\n",
    "conv2d\\_3 (Conv2D) (None, 29, 29, 32) 9248\n",
    "\n",
    "max\\_pooling2d\\_3 (MaxPooling (None, 14, 14, 32) 0\n",
    "\n",
    "2D)\n",
    "\n",
    "flatten\\_1 (Flatten) (None, 6272) 0\n",
    "\n",
    "dense\\_2 (Dense) (None, 128) 802944\n",
    "\n",
    "dense\\_3 (Dense) (None, 4) 516\n",
    "\n",
    "=================================================================\n",
    "\n",
    "Total params: 813,604\n",
    "\n",
    "Trainable params: 813,604\n",
    "\n",
    "Non-trainable params: 0\n",
    "\n",
    "\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\n",
    "\n",
    "In \\[20\\]:\n",
    "\n",
    "*\\# Compiling the model*\n",
    "\n",
    "model**.**compile(optimizer**=**'adam',\n",
    "loss**=**'categorical\\_crossentropy', metrics**=**\\['accuracy'\\])\n",
    "\n",
    "In \\[21\\]:\n",
    "\n",
    "*\\# Fitting the model*\n",
    "\n",
    "model**.**fit\\_generator(generator**=**x\\_train,steps\\_per\\_epoch**=**len(x\\_train),epochs**=**20,validation\\_data**=**x\\_test,validation\\_steps**=**len(x\\_test))\n",
    "\n",
    "/usr/local/lib/python3.7/dist-packages/ipykernel\\_launcher.py:3:\n",
    "UserWarning: \\`Model.fit\\_generator\\` is deprecated and will be removed\n",
    "in a future version. Please use \\`Model.fit\\`, which supports\n",
    "generators.\n",
    "\n",
    "This is separate from the ipykernel package so we can avoid doing\n",
    "imports until\n",
    "\n",
    "Epoch 1/20\n",
    "\n",
    "149/149 \\[==============================\\] - 235s 2s/step - loss: 1.2298\n",
    "- accuracy: 0.4355 - val\\_loss: 1.2029 - val\\_accuracy: 0.4343\n",
    "\n",
    "Epoch 2/20\n",
    "\n",
    "149/149 \\[==============================\\] - 28s 187ms/step - loss:\n",
    "0.9725 - accuracy: 0.5860 - val\\_loss: 1.0199 - val\\_accuracy: 0.5657\n",
    "\n",
    "Epoch 3/20\n",
    "\n",
    "149/149 \\[==============================\\] - 28s 185ms/step - loss:\n",
    "0.7891 - accuracy: 0.6909 - val\\_loss: 0.9436 - val\\_accuracy: 0.5556\n",
    "\n",
    "Epoch 4/20\n",
    "\n",
    "149/149 \\[==============================\\] - 27s 183ms/step - loss:\n",
    "0.7616 - accuracy: 0.6586 - val\\_loss: 0.8652 - val\\_accuracy: 0.6061\n",
    "\n",
    "Epoch 5/20\n",
    "\n",
    "149/149 \\[==============================\\] - 27s 183ms/step - loss:\n",
    "0.6975 - accuracy: 0.7097 - val\\_loss: 0.7742 - val\\_accuracy: 0.6717\n",
    "\n",
    "Epoch 6/20\n",
    "\n",
    "149/149 \\[==============================\\] - 29s 194ms/step - loss:\n",
    "0.6322 - accuracy: 0.7325 - val\\_loss: 0.6910 - val\\_accuracy: 0.7020\n",
    "\n",
    "Epoch 7/20\n",
    "\n",
    "149/149 \\[==============================\\] - 28s 186ms/step - loss:\n",
    "0.6149 - accuracy: 0.7500 - val\\_loss: 1.0359 - val\\_accuracy: 0.6162\n",
    "\n",
    "Epoch 8/20\n",
    "\n",
    "149/149 \\[==============================\\] - 27s 183ms/step - loss:\n",
    "0.5845 - accuracy: 0.7715 - val\\_loss: 0.7130 - val\\_accuracy: 0.7273\n",
    "\n",
    "Epoch 9/20\n",
    "\n",
    "149/149 \\[==============================\\] - 27s 185ms/step - loss:\n",
    "0.5708 - accuracy: 0.7608 - val\\_loss: 0.9826 - val\\_accuracy: 0.6515\n",
    "\n",
    "Epoch 10/20\n",
    "\n",
    "149/149 \\[==============================\\] - 29s 194ms/step - loss:\n",
    "0.5512 - accuracy: 0.7890 - val\\_loss: 0.8693 - val\\_accuracy: 0.6717\n",
    "\n",
    "Epoch 11/20\n",
    "\n",
    "149/149 \\[==============================\\] - 27s 184ms/step - loss:\n",
    "0.5503 - accuracy: 0.7836 - val\\_loss: 0.7070 - val\\_accuracy: 0.7374\n",
    "\n",
    "Epoch 12/20\n",
    "\n",
    "149/149 \\[==============================\\] - 27s 184ms/step - loss:\n",
    "0.4977 - accuracy: 0.8011 - val\\_loss: 0.6304 - val\\_accuracy: 0.7929\n",
    "\n",
    "Epoch 13/20\n",
    "\n",
    "149/149 \\[==============================\\] - 27s 184ms/step - loss:\n",
    "0.4205 - accuracy: 0.8441 - val\\_loss: 0.6953 - val\\_accuracy: 0.7626\n",
    "\n",
    "Epoch 14/20\n",
    "\n",
    "149/149 \\[==============================\\] - 27s 184ms/step - loss:\n",
    "0.4129 - accuracy: 0.8454 - val\\_loss: 0.7047 - val\\_accuracy: 0.7576\n",
    "\n",
    "Epoch 15/20\n",
    "\n",
    "149/149 \\[==============================\\] - 29s 190ms/step - loss:\n",
    "0.4337 - accuracy: 0.8387 - val\\_loss: 0.5759 - val\\_accuracy: 0.8434\n",
    "\n",
    "Epoch 16/20\n",
    "\n",
    "149/149 \\[==============================\\] - 27s 184ms/step - loss:\n",
    "0.3875 - accuracy: 0.8602 - val\\_loss: 0.6379 - val\\_accuracy: 0.7677\n",
    "\n",
    "Epoch 17/20\n",
    "\n",
    "149/149 \\[==============================\\] - 27s 184ms/step - loss:\n",
    "0.3608 - accuracy: 0.8817 - val\\_loss: 0.7366 - val\\_accuracy: 0.7778\n",
    "\n",
    "Epoch 18/20\n",
    "\n",
    "149/149 \\[==============================\\] - 30s 199ms/step - loss:\n",
    "0.3530 - accuracy: 0.8589 - val\\_loss: 0.8593 - val\\_accuracy: 0.7323\n",
    "\n",
    "Epoch 19/20\n",
    "\n",
    "149/149 \\[==============================\\] - 27s 184ms/step - loss:\n",
    "0.3634 - accuracy: 0.8602 - val\\_loss: 0.7928 - val\\_accuracy: 0.7475\n",
    "\n",
    "Epoch 20/20\n",
    "\n",
    "149/149 \\[==============================\\] - 27s 184ms/step - loss:\n",
    "0.2910 - accuracy: 0.8952 - val\\_loss: 0.7734 - val\\_accuracy: 0.8030\n",
    "\n",
    "Out\\[21\\]:\n",
    "\n",
    "In \\[22\\]:\n",
    "\n",
    "*\\# Save the model*\n",
    "\n",
    "model**.**save('naturaldisaster.h5')\n",
    "\n",
    "model\\_json **=** model**.**to\\_json()\n",
    "\n",
    "**with** open(\"model-bw.json\", \"w\") **as** json\\_file:\n",
    "\n",
    "json\\_file**.**write(model\\_json)\n",
    "\n",
    "In \\[23\\]:\n",
    "\n",
    "*\\# Load the saved model*\n",
    "\n",
    "**from** tensorflow.keras.models **import** load\\_model\n",
    "\n",
    "**from** tensorflow.keras.preprocessing **import** image\n",
    "\n",
    "model **=** load\\_model('naturaldisaster.h5')\n",
    "\n",
    "In \\[24\\]:\n",
    "\n",
    "x\\_train**.**class\\_indices\n",
    "\n",
    "Out\\[24\\]:\n",
    "\n",
    "{'Cyclone': 0, 'Earthquake': 1, 'Flood': 2, 'Wildfire': 3}\n",
    "\n",
    "In \\[25\\]:\n",
    "\n",
    "*\\# taking image as input*\n",
    "\n",
    "img **=**\n",
    "image**.**load\\_img('/content/drive/MyDrive/dataset/test\\_set/Earthquake/1333.jpg',target\\_size**=**(64,64))\n",
    "\n",
    "x**=**image**.**img\\_to\\_array(img)\n",
    "\n",
    "x**=**np**.**expand\\_dims(x,axis**=**0)\n",
    "\n",
    "index**=**\\['Cyclone','Earthquake','Flood','Wildfire'\\]\n",
    "\n",
    "y**=**np**.**argmax(model**.**predict(x),axis**=**1)\n",
    "\n",
    "print(index\\[int(y)\\])\n",
    "\n",
    "1/1 \\[==============================\\] - 0s 93ms/step\n",
    "\n",
    "Earthquake\n",
    "\n",
    "In \\[30\\]:\n",
    "\n",
    "*\\# input 2*\n",
    "\n",
    "img **=**\n",
    "image**.**load\\_img('/content/drive/MyDrive/dataset/test\\_set/Flood/1009.jpg',target\\_size**=**(64,64))\n",
    "\n",
    "x**=**image**.**img\\_to\\_array(img)\n",
    "\n",
    "x**=**np**.**expand\\_dims(x,axis**=**0)\n",
    "\n",
    "index**=**\\['Cyclone','Earthquake','Flood','Wildfire'\\]\n",
    "\n",
    "y**=**np**.**argmax(model**.**predict(x),axis**=**1)\n",
    "\n",
    "print(index\\[int(y)\\])\n",
    "\n",
    "1/1 \\[==============================\\] - 0s 16ms/step\n",
    "\n",
    "Flood\n",
    "\n",
    "In \\[27\\]:\n",
    "\n",
    "*\\# input 3*\n",
    "\n",
    "img **=**\n",
    "image**.**load\\_img('/content/drive/MyDrive/dataset/test\\_set/Wildfire/1065.jpg',target\\_size**=**(64,64))\n",
    "\n",
    "x**=**image**.**img\\_to\\_array(img)\n",
    "\n",
    "x**=**np**.**expand\\_dims(x,axis**=**0)\n",
    "\n",
    "index**=**\\['Cyclone','Earthquake','Flood','Wildfire'\\]\n",
    "\n",
    "y**=**np**.**argmax(model**.**predict(x),axis**=**1)\n",
    "\n",
    "print(index\\[int(y)\\])\n",
    "\n",
    "1/1 \\[==============================\\] - 0s 14ms/step\n",
    "\n",
    "Wildfire\n",
    "\n",
    "In \\[28\\]:\n",
    "\n",
    "*\\# input 4*\n",
    "\n",
    "img **=**\n",
    "image**.**load\\_img('/content/drive/MyDrive/dataset/test\\_set/Cyclone/903.jpg',target\\_size**=**(64,64))\n",
    "\n",
    "x**=**image**.**img\\_to\\_array(img)\n",
    "\n",
    "x**=**np**.**expand\\_dims(x,axis**=**0)\n",
    "\n",
    "index**=**\\['Cyclone','Earthquake','Flood','Wildfire'\\]\n",
    "\n",
    "y**=**np**.**argmax(model**.**predict(x),axis**=**1)\n",
    "\n",
    "print(index\\[int(y)\\])\n",
    "\n",
    "1/1 \\[==============================\\] - 0s 14ms/step\n",
    "\n",
    "Cyclone"
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
